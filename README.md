# CUDA Programming Notes ğŸ“

## ğŸ“š Resources

- [Modal GPU Glossary](https://modal.com/gpu-glossary)
- [Triton Tutorials](https://triton-lang.org/main/getting-started/tutorials/)


## General concepts

* A kernel is the unit of CUDA code that programmers typically write and compose, akin to a procedure or function in languages targeting CPUs.
* A thread block is a level of the CUDA programming model's thread hierarchy below a grid but above aÂ warpÂ . It is the CUDA programming model's abstract equivalent of the concreteÂ cooperative thread arraysÂ Â inÂ PTXÂ /SASSÂ .]
* At the highest level, multipleÂ thread blocksÂ Â are organized into aÂ thread block gridÂ Â that spans the entire GPU.Â Thread blocksÂ Â are strictly limited in their coordiation and communication.
* Â threadsÂ Â execute on individualÂ coresÂ ,Â thread blocksÂ Â are scheduled ontoÂ SMsÂ , andÂ gridsÂ Â utilize all availableÂ SMsÂ Â on the device.
* Shared memory is the level of theÂ memory hierarchyÂ Â corresponding to theÂ thread blockÂ Â level of the thread group hierarchy in theÂ CUDA programming modelÂ . It is generally expected to be much smaller but much faster (in throughput and latency) than theÂ global memoryÂ .


## Vector Reduction 

* On the host, they are PyTorch tensors. When you call the kernel, Triton passes device pointers to their first elements to the JITâ€™ed kernel. Inside the kernel you indeed work with pointers.
* num_programs is the size of the grid along axis 0 (so your grid is (num_programs,)). A grid can be `1D/2D/3D`, but here itâ€™s 1D.
* In Triton, `BLOCK_SIZE` is not a thread count. Itâ€™s a compile-time tile size = number of elements each program processes (drives the shape of tl.arange, masks, vector loads/stores).
* in Triton, num_warps sets how many hardware threads run a program (block), while BLOCK_SIZE sets how many data elements that program processes.

### How they combine

* Threads per program (CTA) = num_warps Ã— 32
e.g., `num_warps=4` â†’ 128 threads.

* `Elements per program = BLOCK_SIZE`
e.g., BLOCK_SIZE=512 â†’ 512 elements handled by that same program.

So with `num_warps=4` and `BLOCK_SIZE=512`, you have 128 threads processing 512 elements.
On average: 4 elements per thread (`512 / 128 = 4`).
